{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<div style = \"text-align: center\">MNM: A Per-Task Convolutional Neural Network System</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook describes a model that I designed in order to avoid catastrophic interference during online, incremental task learning.\n",
      "\n",
      "~ Joey L. Maalouf"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Model Theory"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The model is based on the concept of creating a convolutional neural network for each of the tasks that our model trains on, for use as a binary classifier. Given some initial batch of data, the model creates binarized versions of the class labels for each of the classes it contains, and trains a new network for each one. It does the same for any new tasks in any future data, ensuring that any class presented to the model, no matter when, will have a network devoted to it.\n",
      "\n",
      "The model predicts output by running the input through each of its internal class convnets and, for each net, outputting a probability (on the scale of 0-1) that the input is of the class corresponding to that net. The model checks which net has the highest probability and returns its corresponding class as the output.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style = \"text-align: center\"><img src = \"https://raw.githubusercontent.com/joeylmaalouf/conv-net-ella/master/notebooks/mnm.png\" /></div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Model Demonstration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I chose to use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset for my experimentation. It contains 10 classes (the handwritten digits 0-9). For demonstration purposes, I use classes 0-7 as the initial data and I introduce classes 8 and 9 as incremental additions afterwards."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pwd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "u'/home/jmaalouf/conv-net-ella/notebooks'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls .."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[0m\u001b[01;34mconv_net\u001b[0m/  \u001b[01;34mmodern_net\u001b[0m/  \u001b[01;34mnotebooks\u001b[0m/  README.md  \u001b[01;34mros_workspace\u001b[0m/\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ../conv_net/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/jmaalouf/conv-net-ella/conv_net\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run multi-convnet-model.py -h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Usage: multi-convnet-model.py [options]\n",
        "\n",
        "Options:\n",
        "  -h, --help            show this help message and exit\n",
        "  -v, --verbose         print non-essential output to stdout\n",
        "  -t, --test            run additional per-task accuracy tests\n",
        "  -e EPOCHS, --epochs=EPOCHS\n",
        "                        number of epochs for net training\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run multi-convnet-model.py -e 5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Batch training model on starting tasks 0-7...\n",
        "Accuracy on tasks 0-7: 0.9920"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Incrementally training model on new task 8...\n",
        "Accuracy on tasks 0-8: 0.9908"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Incrementally training model on new task 9...\n",
        "Accuracy on tasks 0-9: 0.9878"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see from the output above, the model is quite accurate, but unfortunately still suffers from catastrophic interference; note how the accuracy decreases with each new task introduced. This decrease is not due to the standard interference of weights shifting during further training; it is due to the batch-trained binary models encountering digits during incremental task learning that they were never trained on, and therefore being over- or under-confident in their predictions.\n",
      "\n",
      "For reference, let's look at how a fairly standard convolutional neural network does on MNIST:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run convnet.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.9403"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.9694"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.98"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.9848"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.9879"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Program complete.\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our model has very comparable accuracy; the standard convnet reaches 98.79% accuracy on its fifth epoch, while our custom model gets to 98.78%."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Solution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I realized a possible fix for the problem described above: since the data we use as negative examples for the new tasks are positive examples for their own classes, we can train new nets for all of the classes that we see, instead of just the ones that we've never seen before. We then combine the predicted probabilities for multiple nets of the same class by taking the average. We can see this process in the stacked version of the model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run multi-convnet-model-stacked.py -e 5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Batch training model on starting tasks 0-7...\n",
        "Accuracy on tasks 0-7: 0.9916"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Incrementally training model on new task 8...\n",
        "Accuracy on tasks 0-8: 0.9933"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Incrementally training model on new task 9...\n",
        "Accuracy on tasks 0-9: 0.9922"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interestingly enough, this model actually improves in accuracy when training on the first new task, likely due to the additional nets trained for all of the classes, but then decreases somewhat on the next one. However, when we compare the initial accuracy on the batch data to the final accuracy on the full set of tasks, we still see an improvement, unlike in our non-stacking model. The one downside of this solution is that, as we can see from the timer output above, it is slower to run due to the number of networks we use."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}